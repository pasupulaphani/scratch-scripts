{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "449e8dac-7a22-4ee0-9591-c8b8e92a0577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/new_work/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "[nltk_data] Downloading package punkt to /Users/new_work/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/new_work/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import textstat\n",
    "from transformers import pipeline\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiment_model = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "financial_terms = [\"growth\", \"risk\", \"guidance\", \"margin\", \"revenue\", \"cash flow\", \"EPS\", \"outlook\"]\n",
    "positive_words = set([\"strong\", \"growth\", \"positive\", \"profit\", \"beat\", \"up\", \"optimistic\", \"confident\"])\n",
    "negative_words = set([\"decline\", \"loss\", \"negative\", \"down\", \"miss\", \"risk\", \"uncertain\", \"challenge\"])\n",
    "\n",
    "def count_forward_looking_statements(text):\n",
    "    patterns = [r\"\\bexpect\\b\", r\"\\bforecast\\b\", r\"\\bproject\\b\", r\"\\bwill\\b\", r\"\\blooking ahead\\b\"]\n",
    "    return sum(len(re.findall(pat, text, re.IGNORECASE)) for pat in patterns)\n",
    "\n",
    "def extract_sections(text):\n",
    "    \"\"\"Roughly split transcript into opening, Q&A, and closing.\"\"\"\n",
    "    qna_start = re.search(r\"Q&A|Question-and-Answer|Questions and Answers\", text, re.IGNORECASE)\n",
    "    closing_start = re.search(r\"Closing Remarks|Final Remarks\", text, re.IGNORECASE)\n",
    "\n",
    "    start_qna = qna_start.start() if qna_start else int(0.6 * len(text))\n",
    "    start_close = closing_start.start() if closing_start else int(0.85 * len(text))\n",
    "\n",
    "    return {\n",
    "        \"opening\": text[:start_qna],\n",
    "        \"qna\": text[start_qna:start_close],\n",
    "        \"closing\": text[start_close:]\n",
    "    }\n",
    "\n",
    "def sentiment_scores(text):\n",
    "    return sid.polarity_scores(text)\n",
    "\n",
    "def sentiment_volatility(sentences):\n",
    "    scores = [sid.polarity_scores(s)[\"compound\"] for s in sentences]\n",
    "    return {\n",
    "        \"variance\": float(np.var(scores)),\n",
    "        \"shift\": float(scores[-1] - scores[0]) if scores else 0.0\n",
    "    }\n",
    "\n",
    "def extract_features(transcript):\n",
    "    sections = extract_sections(transcript)\n",
    "    sentences = nltk.sent_tokenize(transcript)\n",
    "\n",
    "    # Sentiment scores\n",
    "    total_sentiment = sentiment_scores(transcript)\n",
    "    section_sentiments = {k: sentiment_scores(v) for k, v in sections.items()}\n",
    "\n",
    "    # Sentiment volatility\n",
    "    volatility = sentiment_volatility(sentences)\n",
    "\n",
    "    # Count words\n",
    "    word_tokens = nltk.word_tokenize(transcript.lower())\n",
    "    pos_freq = sum(word in positive_words for word in word_tokens)\n",
    "    neg_freq = sum(word in negative_words for word in word_tokens)\n",
    "\n",
    "    # Financial term frequency\n",
    "    term_freq = {term: transcript.lower().count(term) for term in financial_terms}\n",
    "\n",
    "    # Forward-looking statements\n",
    "    fwd_looking_count = count_forward_looking_statements(transcript)\n",
    "\n",
    "    # Readability\n",
    "    readability = {\n",
    "        \"flesch\": textstat.flesch_reading_ease(transcript),\n",
    "        \"gunning_fog\": textstat.gunning_fog(transcript)\n",
    "    }\n",
    "\n",
    "    # Analyst questions\n",
    "    num_questions = transcript.count(\"?\")\n",
    "    question_sentences = [s for s in sentences if \"?\" in s]\n",
    "    question_sentiments = [sid.polarity_scores(q)[\"compound\"] for q in question_sentences]\n",
    "\n",
    "    # Sentiment shift\n",
    "    shift = volatility[\"shift\"]\n",
    "\n",
    "    return {\n",
    "        \"avg_sentiment\": total_sentiment,\n",
    "        \"sentiment_by_section\": section_sentiments,\n",
    "        \"positive_word_freq\": pos_freq,\n",
    "        \"negative_word_freq\": neg_freq,\n",
    "        \"forward_looking_statements\": fwd_looking_count,\n",
    "        \"sentiment_volatility\": volatility[\"variance\"],\n",
    "        \"sentiment_shift\": shift,\n",
    "        \"financial_term_freq\": term_freq,\n",
    "        \"num_questions\": len(question_sentences),\n",
    "        \"avg_question_sentiment\": np.mean(question_sentiments) if question_sentiments else 0,\n",
    "        \"readability\": readability,\n",
    "        \"transcript_length\": len(word_tokens),\n",
    "        \"guidance_mentions\": transcript.lower().count(\"guidance\"),\n",
    "        \"surprise_announcement\": \"surprise\" in transcript.lower() or \"unexpected\" in transcript.lower()\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8da01d2b-299a-4cae-b95d-abe2bf252199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved to: data/appended_with_sentiment_features.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "transcripts_df = pd.read_csv('data/transcripts_train.csv')\n",
    "stock_prices_df = pd.read_csv('data/stock_prices_train.csv')\n",
    "\n",
    "# Merge the DataFrames on a common column 'company' and 'date'\n",
    "stock_prices_and_call_transcripts_df = pd.merge(transcripts_df, stock_prices_df, on=['company', 'date'], how='inner')\n",
    "\n",
    "\n",
    "\n",
    "def flatten_dict(d, parent_key='', sep='_'):\n",
    "    \"\"\"Flatten nested dictionary.\"\"\"\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "\n",
    "\n",
    "feature_rows = []\n",
    "\n",
    "for idx, row in stock_prices_and_call_transcripts_df.iterrows():\n",
    "    transcript = row['transcript']\n",
    "    \n",
    "    try:\n",
    "        # Extract and flatten features\n",
    "        features = extract_features(transcript)\n",
    "        flat_features = flatten_dict(features)\n",
    "        feature_rows.append(flat_features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {idx}: {e}\")\n",
    "        feature_rows.append({})  # Placeholder if something breaks\n",
    "\n",
    "features_df = pd.DataFrame(feature_rows)\n",
    "\n",
    "df_with_sentiment_features = pd.concat([stock_prices_and_call_transcripts_df.reset_index(drop=True), features_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\n",
    "df_with_sentiment_features.head()\n",
    "\n",
    "output_file = \"data/appended_with_sentiment_features.csv\"\n",
    "df_with_sentiment_features.to_csv(output_file, index=False)\n",
    "print(f\"Merged file saved to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc77a0a-10f7-45e5-99b5-af060673c794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3] *",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
